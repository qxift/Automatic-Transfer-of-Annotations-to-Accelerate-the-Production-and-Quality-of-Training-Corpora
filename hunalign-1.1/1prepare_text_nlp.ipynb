{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for text processing and data handling\n",
    "import os  # File system operations\n",
    "import pickle  # Saving and loading Python objects\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_novel(input_file, output_file, language):\n",
    "    \"\"\"\n",
    "    Tokenize a text file into sentences using a language-specific tokenizer.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file: Path to the input text file.\n",
    "    - output_file: Path to save the tokenized sentences.\n",
    "    - language: Language of the text ('french', 'english', or 'russian').\n",
    "    \"\"\"\n",
    "\n",
    "    # Select the appropriate tokenizer for the specified language\n",
    "    if language == 'french':\n",
    "        tokenizer_path = 'punkt/french.pickle'\n",
    "    elif language == 'english':\n",
    "        tokenizer_path = 'punkt/english.pickle'\n",
    "    elif language == \"russian\":\n",
    "        tokenizer_path = 'punkt/russian.pickle'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {language}\")  # Handle unsupported languages\n",
    "\n",
    "    # Load the tokenizer from the pickle file\n",
    "    with open(tokenizer_path, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "\n",
    "    # Read and preprocess the text from the input file\n",
    "    with open(input_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "    text = text.replace(\"\\n\", \" \")  # Replace line breaks with spaces\n",
    "    text = ' '.join(text.split())  # Normalize multiple spaces to single\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    processed_sentences = tokenizer.tokenize(text)\n",
    "\n",
    "    # Save the tokenized sentences to the output file, one sentence per line\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(processed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode  # For normalizing text by removing accents\n",
    "\n",
    "# Define directories and novel name\n",
    "data_folder = \"data\"  # Folder for input text files, switch to \"data_test\" for testing\n",
    "result_folder = \"res\"  # Folder to save tokenized output files\n",
    "novel = \"alice\"  # Base name of the text files (no language suffix)\n",
    "novel_fr = novel + \"_fr.txt\"  # French version of the novel\n",
    "novel_eng = novel + \"_eng.txt\"  # English version of the novel\n",
    "\n",
    "input_file_fr = os.path.join(data_folder, novel, novel_fr)  # French input file\n",
    "input_file_eng = os.path.join(data_folder, novel, novel_eng) # English input file\n",
    "output_file_fr = os.path.join(result_folder, novel, novel_fr)  # French output file\n",
    "output_file_eng = os.path.join(result_folder, novel, novel_eng)  # English output file\n",
    "\n",
    "# Normalize French text by removing accents\n",
    "with open(input_file_fr, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "normalized_lines = [unidecode(line) for line in lines]  # Remove accents line by line\n",
    "\n",
    "# Overwrite the original French file with normalized text\n",
    "with open(input_file_fr, 'w') as file:\n",
    "    file.writelines(normalized_lines)\n",
    "\n",
    "# Tokenize and save sentences for English and French texts\n",
    "tokenize_novel(input_file_eng, output_file_eng, \"english\")  # Tokenize English file\n",
    "tokenize_novel(input_file_fr, output_file_fr, \"french\")  # Tokenize French file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Hunalign for Text Alignment\n",
    "\n",
    "Hunalign is a tool for aligning bilingual text files. It uses a dictionary file to improve accuracy, taking the following inputs:\n",
    "\n",
    "1. **Dictionary file**: A bilingual dictionary to guide alignment.\n",
    "2. **Source text file**: The first text file (e.g., English).\n",
    "3. **Target text file**: The second text file (e.g., French).\n",
    "4. **Output file**: The aligned sentences output file.\n",
    "\n",
    "## Full Dataset Script\n",
    "\n",
    "Use this command to align the full dataset and save the result to `accuracy/<novel_folder>/aligned_output.txt`:\n",
    "\n",
    "```bash\n",
    "# Full dataset alignment for alice, remember wherever it says alice to write your novel file name instead\n",
    "src/hunalign/hunalign res/eng-fra.dic res/alice/alice_eng.txt res/alice/alice_fr.txt -text > accuracy/alice/aligned_output.txt\n",
    "\n",
    "# Test dataset alignment\n",
    "src/hunalign/hunalign res/eng-fra.dic res/alice/alice_test_eng.txt res/alice/alice_test_fr.txt -text > accuracy/alice/test_aligned_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
